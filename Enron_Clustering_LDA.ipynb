{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Latent Dirichlet Allocation Analysis\n",
    "<hr>\n",
    "**Author: ** *Gilberto Diaz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('enron_lda').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('./user_and_emails.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|   user|                From|          email_body|\n",
      "+-------+--------------------+--------------------+\n",
      "|allen-p|phillip.allen@enr...|Here is our forec...|\n",
      "|allen-p|phillip.allen@enr...|Traveling to have...|\n",
      "|allen-p|phillip.allen@enr...|test successful  ...|\n",
      "|allen-p|phillip.allen@enr...|Randy    Can you ...|\n",
      "|allen-p|phillip.allen@enr...|Let s shoot for T...|\n",
      "|allen-p|phillip.allen@enr...|Greg    How about...|\n",
      "|allen-p|phillip.allen@enr...|Please cc the fol...|\n",
      "|allen-p|phillip.allen@enr...|any morning betwe...|\n",
      "|allen-p|phillip.allen@enr...|1  login   pallen...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|\n",
      "|allen-p|phillip.allen@enr...|Mr  Buckner    Fo...|\n",
      "|allen-p|phillip.allen@enr...|Lucy    Here are ...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|\n",
      "|allen-p|phillip.allen@enr...|Dave     Here are...|\n",
      "|allen-p|phillip.allen@enr...|Paula    35 milli...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|\n",
      "|allen-p|phillip.allen@enr...|Tim   mike grigsb...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (StopWordsRemover, \n",
    "                                Tokenizer, \n",
    "                                CountVectorizer, \n",
    "                                RegexTokenizer,\n",
    "                                IDF)\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|   user|                From|          email_body|\n",
      "+-------+--------------------+--------------------+\n",
      "|allen-p|phillip.allen@enr...|Here is our forec...|\n",
      "|allen-p|phillip.allen@enr...|Traveling to have...|\n",
      "|allen-p|phillip.allen@enr...|test successful  ...|\n",
      "|allen-p|phillip.allen@enr...|Randy    Can you ...|\n",
      "|allen-p|phillip.allen@enr...|Let s shoot for T...|\n",
      "|allen-p|phillip.allen@enr...|Greg    How about...|\n",
      "|allen-p|phillip.allen@enr...|Please cc the fol...|\n",
      "|allen-p|phillip.allen@enr...|any morning betwe...|\n",
      "|allen-p|phillip.allen@enr...|1  login   pallen...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|\n",
      "|allen-p|phillip.allen@enr...|Mr  Buckner    Fo...|\n",
      "|allen-p|phillip.allen@enr...|Lucy    Here are ...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|\n",
      "|allen-p|phillip.allen@enr...|Dave     Here are...|\n",
      "|allen-p|phillip.allen@enr...|Paula    35 milli...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|\n",
      "|allen-p|phillip.allen@enr...|Tim   mike grigsb...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_id = data.withColumn('id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+---+\n",
      "|   user|                From|          email_body| id|\n",
      "+-------+--------------------+--------------------+---+\n",
      "|allen-p|phillip.allen@enr...|Here is our forec...|  0|\n",
      "|allen-p|phillip.allen@enr...|Traveling to have...|  1|\n",
      "|allen-p|phillip.allen@enr...|test successful  ...|  2|\n",
      "|allen-p|phillip.allen@enr...|Randy    Can you ...|  3|\n",
      "|allen-p|phillip.allen@enr...|Let s shoot for T...|  4|\n",
      "|allen-p|phillip.allen@enr...|Greg    How about...|  5|\n",
      "|allen-p|phillip.allen@enr...|Please cc the fol...|  6|\n",
      "|allen-p|phillip.allen@enr...|any morning betwe...|  7|\n",
      "|allen-p|phillip.allen@enr...|1  login   pallen...|  8|\n",
      "|allen-p|phillip.allen@enr...|                 ...|  9|\n",
      "|allen-p|phillip.allen@enr...|Mr  Buckner    Fo...| 10|\n",
      "|allen-p|phillip.allen@enr...|Lucy    Here are ...| 11|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 12|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 13|\n",
      "|allen-p|phillip.allen@enr...|Dave     Here are...| 14|\n",
      "|allen-p|phillip.allen@enr...|Paula    35 milli...| 15|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 16|\n",
      "|allen-p|phillip.allen@enr...|Tim   mike grigsb...| 17|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 18|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 19|\n",
      "+-------+--------------------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_with_id.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and removing empty tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol='email_body', outputCol='tokens', pattern='\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenized = regex_tokenizer.transform(data_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+---+--------------------+\n",
      "|   user|                From|          email_body| id|              tokens|\n",
      "+-------+--------------------+--------------------+---+--------------------+\n",
      "|allen-p|phillip.allen@enr...|Here is our forec...|  0|[here, is, our, f...|\n",
      "|allen-p|phillip.allen@enr...|Traveling to have...|  1|[traveling, to, h...|\n",
      "|allen-p|phillip.allen@enr...|test successful  ...|  2|[test, successful...|\n",
      "|allen-p|phillip.allen@enr...|Randy    Can you ...|  3|[randy, can, you,...|\n",
      "|allen-p|phillip.allen@enr...|Let s shoot for T...|  4|[let, s, shoot, f...|\n",
      "|allen-p|phillip.allen@enr...|Greg    How about...|  5|[greg, how, about...|\n",
      "|allen-p|phillip.allen@enr...|Please cc the fol...|  6|[please, cc, the,...|\n",
      "|allen-p|phillip.allen@enr...|any morning betwe...|  7|[any, morning, be...|\n",
      "|allen-p|phillip.allen@enr...|1  login   pallen...|  8|[1, login, pallen...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|  9|[forwarded, by, p...|\n",
      "|allen-p|phillip.allen@enr...|Mr  Buckner    Fo...| 10|[mr, buckner, for...|\n",
      "|allen-p|phillip.allen@enr...|Lucy    Here are ...| 11|[lucy, here, are,...|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 12|[forwarded, by, p...|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 13|[forwarded, by, p...|\n",
      "|allen-p|phillip.allen@enr...|Dave     Here are...| 14|[dave, here, are,...|\n",
      "|allen-p|phillip.allen@enr...|Paula    35 milli...| 15|[paula, 35, milli...|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 16|[forwarded, by, p...|\n",
      "|allen-p|phillip.allen@enr...|Tim   mike grigsb...| 17|[tim, mike, grigs...|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 18|[forwarded, by, p...|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 19|[forwarded, by, p...|\n",
      "+-------+--------------------+--------------------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = StopWordsRemover()\n",
    "new_stop_words = [\n",
    "    '00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '0', '1', \n",
    "    '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '15', \n",
    "    '16', '17', '18', '19','20', '25', 'www', 'com', 'pm', '853', 'click',\n",
    "    'mail', 'e', 'http', 'na', 'ees', 'cc', 'hou', 'etc', '30', 'll', '35',\n",
    "    'a','cannot','into','our','thus','about','co','is','ours','to','above',\n",
    "    'could','it','ourselves','together','across','down','its','out','too',\n",
    "    'after','during','itself','over','toward','afterwards','each','last', '50',\n",
    "    'own', 'towards','again','eg','latter','per','under','against','either',\n",
    "    'latterly', 'perhaps','until','all','else','least','rather','up','almost',\n",
    "    'elsewhere', 'less','same','upon','alone','enough','seem','us', 'said',\n",
    "    'along','etc', 'many','seemed','very','already','even','may','seeming','via',\n",
    "    'also','ever', 'me','seems','was','although','every','meanwhile','several',\n",
    "    'always', 'everyone','might','she','well','among','everything','more','should',\n",
    "    'were', 'amongst','everywhere','moreover','since','what','an','except','most',\n",
    "    'whatever','and','few','mostly','some','when','another','first','much',\n",
    "    'somehow','whence','any','for','must','someone','whenever','anyhow', 'mr', \n",
    "    'my','something','where','anyone','formerly','myself','sometime', 'gif', '31',\n",
    "    'whereafter','anything','from','namely','sometimes','whereas','anywhere', \n",
    "    'further','neither','somewhere','whereby','are','had','never','still', 'let',\n",
    "    'wherein','around','has','nevertheless','such','whereupon','as','have', 'ip',\n",
    "    'next','than','wherever','at','he','no','that','whether','be','hence',\n",
    "    'nobody','the','whither','became','her','none','their','which','because', 'send',\n",
    "    'here','noone','them','while','become','hereafter','nor','themselves','who',\n",
    "    'becomes','hereby','not','then','whoever','becoming','herein','nothing',\n",
    "    'thence','whole','been','hereupon','now','there','whom','before','hers', 'ski',\n",
    "    'nowhere','thereafter','whose','beforehand','herself','of','thereby','why', \n",
    "    'especially', 'image', 're', 'we', 'so', 'static', 'width', 'href', '000',\n",
    "    'behind','him','off','therefore','will','being','himself','often','therein', \n",
    "    'with','below','his','on','thereupon','within','beside','how','once', 'try', \n",
    "    'these','without','besides','however','one','they','would','between','i', 'far',\n",
    "    'only','this','yet','beyond','ie','onto','those','you','both','if','or', 'get',\n",
    "    'though','your','but','in','other','through','yours','by','inc','others', 'suggest', \n",
    "    'take', 'throughout','yourself','can','indeed','otherwise','thru','yourselves', \n",
    "    'login', 'please', 'forwarded', 'pw', 'k', '-', '+', '|', ' ', 'go', 'takes', \n",
    "    'td', 'font', 'br', 'b', 'tr', 'm', 'align', 'net', '3d', '2001', 'new', \n",
    "    'said', '11', 'ect', '2000', 'sent', 'know', 'dbcaps97data',\n",
    "    '12', 'need', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '12', 'aol',\n",
    "    '2002', 'mailto', '713', 'error', 'nbsp', 'et', \n",
    "    'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
    "    ]\n",
    "\n",
    "all_stop_words = stop_words.getStopWords() + new_stop_words\n",
    "stop_words_set = set(all_stop_words)\n",
    "stop_words_set = list(stop_words_set)\n",
    "\n",
    "remover = StopWordsRemover(inputCol='tokens', outputCol='words', stopWords=stop_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = remover.transform(regex_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(inputCol='words', outputCol='vectors') # to user with IDF\n",
    "cv = CountVectorizer(inputCol='words', outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_model = cv.fit(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = count_vectorizer_model.transform(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = count_vectorizer_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idf = IDF(inputCol='vectors', outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idf_model = idf.fit(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rescale_data = idf_model.transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+---+--------------------+--------------------+--------------------+\n",
      "|   user|                From|          email_body| id|              tokens|               words|            features|\n",
      "+-------+--------------------+--------------------+---+--------------------+--------------------+--------------------+\n",
      "|allen-p|phillip.allen@enr...|Here is our forec...|  0|[here, is, our, f...|          [forecast]|(262144,[1917],[1...|\n",
      "|allen-p|phillip.allen@enr...|Traveling to have...|  1|[traveling, to, h...|[traveling, busin...|(262144,[6,16,29,...|\n",
      "|allen-p|phillip.allen@enr...|test successful  ...|  2|[test, successful...|[test, successful...|(262144,[59,955,1...|\n",
      "|allen-p|phillip.allen@enr...|Randy    Can you ...|  3|[randy, can, you,...|[randy, schedule,...|(262144,[31,111,1...|\n",
      "|allen-p|phillip.allen@enr...|Let s shoot for T...|  4|[let, s, shoot, f...|[shoot, tuesday, 45]|(262144,[81,403,6...|\n",
      "|allen-p|phillip.allen@enr...|Greg    How about...|  5|[greg, how, about...|[greg, tuesday, t...|(262144,[76,81,46...|\n",
      "|allen-p|phillip.allen@enr...|Please cc the fol...|  6|[please, cc, the,...|[following, distr...|(262144,[0,25,52,...|\n",
      "|allen-p|phillip.allen@enr...|any morning betwe...|  7|[any, morning, be...|           [morning]|(262144,[275],[1.0])|\n",
      "|allen-p|phillip.allen@enr...|1  login   pallen...|  8|[1, login, pallen...|[pallen, ke9davis...|(262144,[8,46,150...|\n",
      "|allen-p|phillip.allen@enr...|                 ...|  9|[forwarded, by, p...|[phillip, allen, ...|(262144,[0,1,2,3,...|\n",
      "|allen-p|phillip.allen@enr...|Mr  Buckner    Fo...| 10|[mr, buckner, for...|[buckner, deliver...|(262144,[0,3,7,43...|\n",
      "|allen-p|phillip.allen@enr...|Lucy    Here are ...| 11|[lucy, here, are,...|[lucy, rentrolls,...|(262144,[282,569,...|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 12|[forwarded, by, p...|[phillip, allen, ...|(262144,[1,6,7,10...|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 13|[forwarded, by, p...|[phillip, allen, ...|(262144,[1,6,7,10...|\n",
      "|allen-p|phillip.allen@enr...|Dave     Here are...| 14|[dave, here, are,...|[dave, names, wes...|(262144,[293,320,...|\n",
      "|allen-p|phillip.allen@enr...|Paula    35 milli...| 15|[paula, 35, milli...|[paula, million, ...|(262144,[44,936,1...|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 16|[forwarded, by, p...|[phillip, allen, ...|(262144,[0,1,4,6,...|\n",
      "|allen-p|phillip.allen@enr...|Tim   mike grigsb...| 17|[tim, mike, grigs...|[tim, mike, grigs...|(262144,[2,33,82,...|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 18|[forwarded, by, p...|[phillip, allen, ...|(262144,[0,1,11,3...|\n",
      "|allen-p|phillip.allen@enr...|                 ...| 19|[forwarded, by, p...|[phillip, allen, ...|(262144,[0,1,4,27...|\n",
      "+-------+--------------------+--------------------+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rescale_data.show()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA(k=10, seed=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lda_model.fit(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.describeTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[0, 1, 4, 8, 38, ...|[0.05377207869946...|\n",
      "|    1|[0, 5, 1, 13, 154...|[0.02173466966236...|\n",
      "|    2|[489, 472, 2756, ...|[0.02855617595264...|\n",
      "|    3|[0, 259, 8, 44, 1...|[0.01020669808007...|\n",
      "|    4|[59, 102, 24, 0, ...|[0.00609482962220...|\n",
      "|    5|[520, 1858, 946, ...|[0.01064719697642...|\n",
      "|    6|[129, 205, 281, 3...|[0.01403991828246...|\n",
      "|    7|[3, 146, 0, 311, ...|[0.00874891997286...|\n",
      "|    8|[0, 1, 9, 28, 6, ...|[0.01265285162653...|\n",
      "|    9|[2, 3, 12, 14, 10...|[0.01177401931700...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_rdd = topics.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_words = topics_rdd\\\n",
    "    .map(lambda row: row['termIndices'])\\\n",
    "    .map(lambda inx_list: [vocab[idx] for idx in inx_list])\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1\n",
      "===============\n",
      "enron\n",
      "subject\n",
      "corp\n",
      "company\n",
      "enron_development\n",
      "thanks\n",
      "meeting\n",
      "business\n",
      "john\n",
      "gas\n",
      "\n",
      "\n",
      "Topic: 2\n",
      "===============\n",
      "enron\n",
      "message\n",
      "subject\n",
      "original\n",
      "intended\n",
      "recipient\n",
      "doc\n",
      "information\n",
      "kay\n",
      "database\n",
      "\n",
      "\n",
      "Topic: 3\n",
      "===============\n",
      "omni\n",
      "1999\n",
      "3dhou\n",
      "3dect\n",
      "forney\n",
      "3djohn\n",
      "ou\n",
      "omniexcludefromview\n",
      "omniduration\n",
      "omnienddatetime\n",
      "\n",
      "\n",
      "Topic: 4\n",
      "===============\n",
      "enron\n",
      "edu\n",
      "company\n",
      "million\n",
      "vince\n",
      "cn\n",
      "subject\n",
      "request\n",
      "business\n",
      "round\n",
      "\n",
      "\n",
      "Topic: 5\n",
      "===============\n",
      "way\n",
      "texas\n",
      "houston\n",
      "enron\n",
      "time\n",
      "like\n",
      "day\n",
      "good\n",
      "tx\n",
      "year\n",
      "\n",
      "\n",
      "Topic: 6\n",
      "===============\n",
      "dec\n",
      "travelocity\n",
      "active\n",
      "prc\n",
      "m0\n",
      "book\n",
      "position\n",
      "enron\n",
      "travel\n",
      "ft\n",
      "\n",
      "\n",
      "Topic: 7\n",
      "===============\n",
      "size\n",
      "class\n",
      "face\n",
      "images\n",
      "arial\n",
      "table\n",
      "kate\n",
      "right\n",
      "border\n",
      "color\n",
      "\n",
      "\n",
      "Topic: 8\n",
      "===============\n",
      "energy\n",
      "ca\n",
      "enron\n",
      "gov\n",
      "org\n",
      "nytimes\n",
      "html\n",
      "subject\n",
      "cpuc\n",
      "pge\n",
      "\n",
      "\n",
      "Topic: 9\n",
      "===============\n",
      "enron\n",
      "subject\n",
      "thanks\n",
      "agreement\n",
      "time\n",
      "email\n",
      "message\n",
      "call\n",
      "attached\n",
      "information\n",
      "\n",
      "\n",
      "Topic: 10\n",
      "===============\n",
      "power\n",
      "energy\n",
      "california\n",
      "state\n",
      "market\n",
      "gas\n",
      "electricity\n",
      "prices\n",
      "price\n",
      "company\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(topics_words):\n",
    "    print(f'Topic: {idx+1}')\n",
    "    print('===============')\n",
    "    for word in topic:\n",
    "        print(word)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I played around with different setups trying to get coherent topics. With the IDF packages, words in topics where all over the place. This current setup gave me the most coherent topics. I'm open to learn how to fine tune passing different parameters.\n",
    "\n",
    "### Topic: 4\n",
    "I will classify this topic as very corporate conversations.\n",
    "\n",
    "### Topic: 3\n",
    "I will classify this topic as meeting conversations since have words such as call, time, attached, and mark.\n",
    "\n",
    "### Topic: 2\n",
    "I don't have any classification for this topic.\n",
    "\n",
    "### Topic: 1\n",
    "I will classify this topic as stock market\n",
    "\n",
    "\n",
    "## TODO:\n",
    "<hr>\n",
    "- Gain more knowledge about the domain for fine tuning stop words.\n",
    "- Find resources to learn how to fine tune by tweaking function's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
